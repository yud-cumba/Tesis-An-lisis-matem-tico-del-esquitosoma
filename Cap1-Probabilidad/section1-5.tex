\section{Esperanza y funciones generadoras}
Para lograr una mayor simplicidad, suele ser mejor describir las probabilidades con algunos valores típicos. Entre aquellos valores, la media o esperanza es la más importante. Su definición va de acuerdo con el concepto común de promedio. \\Las demostraciones de los resultados expuestos en este capítulo se pueden encontrar en
\\\\
Si en una población determinada, $30$ familias tienen $1$ hijo, $10$ familias tienen $2$ hijos , $7$ tienen $3$ hijos y $3$ tienen $4$ hijos. El número total de familias es $50=30+10+7+3$ y el número total de hijos es $83=1(30)+2(10)+3(7)+4(3)$\\
El número promedio de hijos por familia es 
\begin{eqnarray}
    \label{ejm-esperanza-inicio}
    \frac{83}{50}= 1\big(\frac{30}{50}\big)+2\big(\frac{10}{50}\big)+3\big(\frac{7}{50}\big)+4\big(\frac{3}{50}\big)
\end{eqnarray}
Si definimos nuestra variable aleatoria $X=$ \textit{"números de hijos por familia"} y $f$ la función de probabilidad asociada a $X$. Por la definición de probabilidad clásica se obtiene que $f(1)=\frac{30}{50}$, $f(2)=\frac{10}{50}$, $f(3)=\frac{7}{50}$, $f(4)=\frac{3}{50}$ que coincide con los valores expuestos en \ref{ejm-esperanza-inicio} los cuales al reemplazarlos obtenemos que el promedio de hijos por familia es $1f(1)+2f(2)+3f(3)+4f(4)$.\\Una generalización de esta situación induce a la siguiente definición.
\begin{Def}
    Sea $X$ una variable aleatoria discreta que toma los valores $x_1,x_2,x_3,\ldots$ con probabilidades $f(x_1), f(x_2),\ldots$, la esperanza se define por $$E(X)=\sum x_k f(x_k)$$. Con la condición de que la serie converja absolutamente. En este caso decimos que $X$ tiene esperanza finita.
\end{Def}
Intuitivamente, la esperanza de una variable aleatoria $X$ representa la cantidad promedio que se "espera" como resultado de un experimento aleatorio cuando la probabilidad de cada suceso se mantiene constante y el experimento se repite un elevado número de veces. Cabe decir que el valor que toma la esperanza matemática en algunos casos puede no ser "esperado" en el sentido más general de la palabra (el valor de la esperanza puede ser improbable o incluso imposible). 
\begin{Ejm}
    El cálculo del valor esperado de la variable aleatoria nuestro ejemplo estaría dado por
    \begin{eqnarray}
       E(X)= 0\frac{20}{30}+1\frac{6}{30}+2\frac{6}{30}+3\frac{3}{30}=\frac{1}{2}
    \end{eqnarray}
    La multiplicación de cada valor de $x$ por su probabilidad $f(x)$ proporciona los $xf(x)$ valores en la tercera columna. Siguiendo la definición de esperanza, sumamos esta columna,a $x f(x)$, para obtener la esperanza de $\frac{1}{2}$ automóviles vendidos por día. Cabe destacar que $0,5$ no es un valor posible.
\end{Ejm}
\begin{Def}
    \label{def-prob-funcionGeneraProb}
    Consideremos una variable aleatoria discreta $X$, que toma valores en el conjunto de estados $S=\{x_1,x_2,\ldots\}$ con distribución de probabilidad $\{p_k\}_{k\in\N}$, tal que $$p_k=P(X=x_k)$$ 
    La función generadora de probabilidad ($f.g.p.$) $G$ asociada a la variable aleatoria $X$ (o equivalentemente a su distribución $\{p_k\}$) es una función que se define por 
    \begin{eqnarray}
        G_X(s)=E(s^X)=\sum_{k=0}^\infty p_k s^{x_k} \label{def-funcionGeneradoraProb}
    \end{eqnarray} 
    donde,  $|s|\leq 1$.
\end{Def}
Como $\{f(x_k)\thinspace|\thinspace x_k\in S\}$ es una distribución de probabilidad, entonces\\ $\sum_{k=0}^\infty p_k=1$, por lo tanto, la serie definida en (\ref{def-funcionGeneradoraProb}) converge absolutamente para $|s|\leq 1$ pues $$|G(s)|\leq \sum_{k=0}^\infty|s|^k p_k\leq\suma p_k=1 $$ 
Como su nombre lo menciona, la $f.g.p.$ genera la probabilidad asociada a su distribución.
Notamos que al derivar la $f.g.p$ y evaluarlarlo en $0$ se obtiene que
$$G(0)=p_0$$ $$G'(0)=p_1$$ $$G''(0)=2!P_2$$ $$\vdots$$Desde que la serie (\ref{def-funcionGeneradoraProb}) converge absolutamente cuando $|s|\leq 1$, entonces es infinitamente diferenciable en el intervalo de convergencia. En general, por inducción se demuestra que la $k-$ésima derivada de la $f.g.p.$ de $X$ es $$G^{(k)}(0)=k!p_k$$
De esta forma obtenemos que $$p_k=\frac{G^{(k)}(0)}{k!}$$ recuperando lo que inicialmente era nuestra distribución de probabilidad.\\\\
\begin{Ejm}
    Supongamos que un experimento consiste en lanzar una moneda. Los posibles resultados son cara $C$ o sello $S$. Si $X$ es una variable aleatoria tal que $X(C)=1$, $X(S)=0$ con $P(X=0)=\frac{3}{4}$ y $P(X=1)=\frac{1}{4}$
    Por lo tanto la función generadora de probabilidad es
    $$G(s)=\frac{3}{4}+\frac{s}{4}.$$
    Ahora supongamos que no sabemos la distribución de probabilidad al lanzar esta moneda , pero sí se sabe que la función generadora de probabilidad asociada que es $G(s)=\frac{3}{4}+\frac{s}{4}.$\\
    Se verifica la reconstrucción de la función de probabilidad
    $$P(X=0)=G(0)=\frac{3}{4}.$$    $$P(X=1)=G'(0)=\frac{1}{4}.$$
\end{Ejm}
La función generadora de probabilidad determina de manera única a la distribución en el siguiente sentido. Si $X$ y $Y$ tienen la misma distribución de probabilidad, entonces naturalmente $E(X)=E(Y)$ lo cual implica también que $G_X=G_Y$, para valores donde esta esperanza exista.\\
Inversamente, sean $X$ y $Y$ tales que $G_X$ y
$G_Y$ existen y coinciden en algún intervalo no trivial alrededor del cero,
entonces $X$ y $Y$ tienen la misma distribución. Estas y otras propiedades
generales de la $f.g.p.$ se estudian a continuación.
\begin{Prop}
    Sean $X$ e $Y$ variables aleatorias con valores enteros no negativos tales que $G_X(s)$ y $G_Y(s)$ coinciden en algún intervalo alrededor de $s=0$. Entonces $X$ y $Y$ tienen la misma distribución de probabilidad. \label{prop-funcionGenIgual-distribucuionIgual}
\end{Prop}
\begin{Prop}
    Sean $X$  $Y$ independientes con f.g.p. $G_X$ y $G_Y$ respectivamente, entonces $G_{X+Y}(s)=G_X(s)G_Y(s)$
    \label{prop-funcionGenSuma-productoFuncionGen}
\end{Prop}
\begin{Def} 
    La función de probabilidad de dos valores aleatorios $X, Y$ con valores reales discretos en $\N$ es la función $f_{X,Y} :\R^2 \rightarrow [0, 1]$ dada por
    $$f(x,y)=
    \begin{cases}
        P(X=x,\thinspace Y=y), & \mbox{$x,y=1,2,\ldots$}\\
        0,& \mbox{en otro caso}
    \end{cases}$$
    A esta función se le llama función de probabilidad conjunta de las variables $X$ e $Y$.
\end{Def}
Sean $X$ y $Y$ variables aleatorias independientes, con valores enteros no negativos y con distribuciones de probabilidad $\{a_j\}_{j\in\N}$ y $\{b_k\}_{k\in\N}$ respectivamente, donde $a_j=P(X=j)$ y  $b_k=P(Y=k)$ $j,k\in\N$ , entonces $P(X=j,Y=k)=a_j b_k$.\\ La suma $X+Y$ es una nueva variable aleatoria por la proposición (\ref{prop-variableAl-X+YesVA}), y se cumple que $$(X+Y=m)=\bigcup_{j=0}^{m}(X=j,Y=m-j)$$ \\ Por lo tanto la distribución de la variable aleatoria $X+Y$, $\{c_m\}_{m\in\N}$, donde $c_m=P(X+Y=m)$, viene dada por
\begin{eqnarray}
    c_m=\sum_{j=0}^m P(X=j,Y=m-j)=\sum_{j=0}^m a_j b_{m-j}\label{def-convolucion}
\end{eqnarray}
La nueva distribución de probabilidad \ref{def-convolucion} que parte de las dos sucesiones $\{a_k\}$ y $\{b_k\}$ y conduce a una nueva sucesión $\{c_k\}$ ocurre con tanta frecuencia que es conveniente darle un nombre y una notación especial para ella
\begin{Def}
    Sean $\{a_k\}$ y $\{b_k\}$ dos sucesiones de enteros no negativos (no necesariamente distribuciones de probabilidades). La nueva sucesión $\{c_m\}$ definida en \ref{def-convolucion}, se llamará convolución de $a_k$ y $b_k$, y se denotará por
    \begin{eqnarray}
        \{c_k\}=\{a_k\}*\{b_k\}
        \label{def-notaciónConvolucion}
    \end{eqnarray}
\end{Def}
\begin{Obs}
    Si $f_X(k)=\{a_k\}$ y $f_Y(k)=\{b_k\}$ son funciones de probabilidad de las variables aleatorias $X$ e $Y$ respectivamente, denotaremos a la convolución $c_k=(f_X*f_Y)(k)$
\end{Obs}
\begin{Ejm}
    Si $a_k=b_k=1$ para toda $k=0,1,2,\ldots$, entonces $c_k=\sum_{j=0}^k 1=k+1$.\\Si $a_k=k$ y $b_k=1$, entonces $c_k=\sum_{j=0}^k j =\frac{k+k+1}{2}$.\\Finalmente, si $a_0=a_1=\frac{1}{2}$ y $a_k=0$ para $k\geq 2$ entonces $c_k=\frac{b_k+b_k-1}{2} $ 
\end{Ejm}